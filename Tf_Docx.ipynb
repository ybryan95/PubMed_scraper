{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94400002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries. Make sure to pip install missing ones.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import pandas as pd\n",
    "import re\n",
    "from docx import Document\n",
    "import docx\n",
    "import time\n",
    "from pymed import PubMed\n",
    "from Bio import Entrez\n",
    "import concurrent.futures\n",
    "import random\n",
    "from pubmed_lookup import PubMedLookup, Publication\n",
    "import scispacy\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "from docx.shared import Inches\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05c183",
   "metadata": {},
   "source": [
    "# Types of Ontologies- params : \"ontologies\" in find_pref_label function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0296622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# import json\n",
    "# import os\n",
    "# from pprint import pprint\n",
    "\n",
    "# REST_URL = \"http://data.bioontology.org\"\n",
    "# API_KEY = \"d9885f69-dfed-44cd-81c3-f0591df67a03\"\n",
    "\n",
    "\n",
    "# def get_json(url):\n",
    "#     opener = urllib.request.build_opener()\n",
    "#     opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "#     return json.loads(opener.open(url).read())\n",
    "\n",
    "# # Get the available resources\n",
    "# resources = get_json(REST_URL + \"/\")\n",
    "\n",
    "# # Get the ontologies from the `ontologies` link\n",
    "# ontologies = get_json(resources[\"links\"][\"ontologies\"])\n",
    "\n",
    "# # Get the name and ontology id from the returned list\n",
    "# ontology_output = []\n",
    "# for ontology in ontologies:\n",
    "#     ontology_output.append(f\"{ontology['name']}\\n{ontology['@id']}\\n\")\n",
    "\n",
    "# # Print the first ontology in the list\n",
    "# pprint(ontologies[0])\n",
    "\n",
    "# # Print the names and ids\n",
    "# print(\"\\n\\n\")\n",
    "# for ont in ontology_output:\n",
    "#     print(ont)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd9acb",
   "metadata": {},
   "source": [
    "# Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust search terms to append to use pubmed advanced search.\n",
    "# For guide on how to make your own ls, refer to query box at\n",
    "# https://pubmed.ncbi.nlm.nih.gov/advanced/\n",
    "def gene_to_search(element, fullName):\n",
    "    ls = []\n",
    "    ls.append(\"(\" + element + \"[Title/Abstract]) AND ((AUTISM[Title/Abstract]) OR (autistic[Title/Abstract])) \\\n",
    "    NOT (CANCER[Title/Abstract]) NOT (TUMOR[Title/Abstract])\")\n",
    "\n",
    "    ls.append(\"(\" + fullName + \"[Title/Abstract]) AND ((AUTISM[Title/Abstract]) OR (autistic[Title/Abstract])) \\\n",
    "    NOT (CANCER[Title/Abstract]) NOT (TUMOR[Title/Abstract])\")\n",
    "        \n",
    "    return ls\n",
    "\n",
    "def gene_fullName(gene_abbr):\n",
    "    url = f'https://mygene.info/v3/query?q=symbol:{gene_abbr}&fields=name'\n",
    "    time.sleep(0.1)\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    return data['hits'][0]['name']\n",
    "\n",
    "def generate_wordcloud(input_text, nlp1, nlp2, nlp3):\n",
    "    doc1 = nlp1(input_text)\n",
    "    doc2 = nlp2(input_text)\n",
    "    doc3 = nlp3(input_text)\n",
    "\n",
    "    gene_pattern = r\"^[A-Z]{1}[A-Za-z0-9_-]*[A-Za-z]{1}[A-Za-z0-9_-]*$\"\n",
    "    genes = [token.text for token in doc2 if re.match(gene_pattern, token.text) and token.pos_ == 'NOUN']\n",
    "    \n",
    "    entities1 = ['_'.join(ent.text.split()) for ent in doc1.ents if ent.label_ in \n",
    "                 {'ORGAN', 'CELL', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'PATHOLOGICAL_FORMATION'}]\n",
    "    entities2 = ['_'.join(ent.text.split()) for ent in doc2.ents if ent.label_ in \n",
    "                 {'DISEASE'}]\n",
    "    entities3 = [ent.text for ent in doc2.ents if ent.label_ in \n",
    "                 {'TAXON'}]\n",
    "    combined_entities = genes + entities1 + entities2 + entities3\n",
    "    \n",
    "    stem_cell_pattern = re.compile(r'\\bstem cell\\b', re.IGNORECASE)\n",
    "    ipsc_pattern = re.compile(r'\\bipsc\\b', re.IGNORECASE)\n",
    "    if re.search(stem_cell_pattern, input_text):\n",
    "        combined_entities.append('stem_cell')\n",
    "    if re.search(ipsc_pattern, input_text):\n",
    "        combined_entities.append('iPSC')\n",
    "\n",
    "    filtered_entities = []\n",
    "    for entity in combined_entities:\n",
    "        if not any([entity in other_entity and entity != other_entity for other_entity in combined_entities]):\n",
    "            filtered_entities.append(entity)\n",
    "\n",
    "    filtered_text = ' '.join(filtered_entities)\n",
    "\n",
    "    if not filtered_text:\n",
    "        img = Image.new('RGB', (400, 200), color='white')\n",
    "        return img\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white', max_words=100, contour_width=3, contour_color='steelblue')\n",
    "    wordcloud.generate(filtered_text)\n",
    "\n",
    "    return wordcloud\n",
    "\n",
    "def fetch_abstract(pmid):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    try:\n",
    "        abstract_sections = records[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"]\n",
    "        abstract = \"\\n\".join(str(section) for section in abstract_sections)\n",
    "    except KeyError:\n",
    "        abstract = \"No abstract available\"\n",
    "    return abstract\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def search_query(query, fullName_list, gene, i):\n",
    "    Entrez.email = \"choyoungb@gmail.com\"  \n",
    "    time.sleep(0.1)\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=1000)\n",
    "    time.sleep(0.1)\n",
    "    record = Entrez.read(handle)\n",
    "    pmid_list = record[\"IdList\"]\n",
    "    article_list = []\n",
    "\n",
    "    for pmid in pmid_list:\n",
    "        try:\n",
    "            time.sleep(0.1)\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\")\n",
    "            time.sleep(0.1)\n",
    "            records = Entrez.read(handle)\n",
    "\n",
    "            try:\n",
    "                title = records[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"ArticleTitle\"]\n",
    "                title = remove_html_tags(title)\n",
    "                pub_date = records[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"Journal\"][\"JournalIssue\"][\"PubDate\"]\n",
    "                article_ids = records[\"PubmedArticle\"][0][\"PubmedData\"][\"ArticleIdList\"]\n",
    "                doi_url = \"NA\"\n",
    "                for article_id in article_ids:\n",
    "                    if article_id.attributes[\"IdType\"] == \"doi\":\n",
    "                        doi_url = \"https://doi.org/\" + article_id\n",
    "                if 'Year' in pub_date:\n",
    "                        year = pub_date['Year']\n",
    "                else:\n",
    "                    year = None\n",
    "                url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}\"\n",
    "                full_abstract = fetch_abstract(pmid)\n",
    "                full_abstract = remove_html_tags(full_abstract)\n",
    "                \n",
    "                article_dict = {'info': \"Url: \" + url + \"\\n\" + \"DOI: \" + doi_url + \"\\n\\n\" + \"Title(\" + year + \"): \"\n",
    "                                + title + \"\\n\\n\" + full_abstract + \"\\n\\n\"}\n",
    "                article_list.append(article_dict)             \n",
    "                \n",
    "            except IndexError:\n",
    "                print(f\"|An error occurred while fetching the article with PMID:{pmid}|\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"|An error occurred while fetching the article with PMID:{pmid}|\")\n",
    "            print(e)\n",
    "\n",
    "    search_df = pd.DataFrame(article_list)\n",
    "    return search_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9cb79",
   "metadata": {},
   "source": [
    "# Load the library for wordcloud-keyword detection\n",
    "* For more info, visit https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9028678/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step takes time\n",
    "nlp1 = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "nlp2 = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "nlp3 = spacy.load(\"en_ner_craft_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2175299",
   "metadata": {},
   "source": [
    "## PubMed Search Engine Description: https://www.nlm.nih.gov/vsearchfaq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a8a9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADNP', 'AHDC1', 'AR', 'ARHGEF2', 'ARID1B', 'ARID2', 'ARNT2', 'ARX', 'ASH1L', 'ATRX', 'BAZ2B', 'BCL11A', 'BRD4', 'BTAF1', 'BTRC', 'CAMK2A', 'CAMTA2', 'CARD11', 'CASZ1', 'CAT', 'CC2D1A', 'CDK8', 'CHAMP1', 'CHD1', 'CHD2', 'CHD3', 'CHD7', 'CHD9', 'CIC', 'CNOT3', 'CREBBP', 'CSDE1', 'CTCF', 'CTNNB1', 'CUL3', 'CUX1', 'CUX2', 'DDX3X', 'DEAF1', 'DEPDC5', 'DLX2', 'DLX3', 'DLX6', 'DNMT3A', 'DVL3', 'EBF3', 'EGR3', 'EN2', 'EP300', 'EP400', 'ERBIN', 'ERG', 'ESR2', 'ESRRB', 'FAN1', 'FBN1', 'FEZF2', 'FOXG1', 'FOXP1', 'FOXP2', 'GLIS1', 'GTF2I', 'HCFC1', 'HDAC4', 'HDAC8', 'HIVEP2', 'HIVEP3', 'HLA-DRB1', 'HMGN1', 'HNRNPD', 'HNRNPK', 'HNRNPU', 'HOXA1', 'IKZF1', 'JARID2', 'KAT2B', 'KAT6A', 'KAT6B', 'KDM2A', 'KDM2B', 'KDM3A', 'KDM5A', 'KDM5B', 'KDM5C', 'KLF16', 'KLF7', 'KMT2A', 'KMT2C', 'LMX1B', 'LZTR1', 'MACF1', 'MAPK3', 'MBD1', 'MBD3', 'MBD4', 'MBD6', 'MECP2', 'MED13', 'MED13L', 'MED23', 'MEF2C', 'MEIS2', 'MET', 'MINK1', 'MKX', 'MNT', 'MSL3', 'MSX2', 'MTF1', 'MYOCD', 'MYT1L', 'NCOA1', 'NCOR1', 'NFE2L3', 'NFIA', 'NFIB', 'NFIX', 'NKX2-2', 'NPAS2', 'NR1D1', 'NR2F1', 'NR3C2', 'NR4A2', 'NSD1', 'NSD2', 'NTRK1', 'OTX1', 'PAX5', 'PAX6', 'PBX1', 'PER1', 'PER2', 'PHF21A', 'PITX1', 'PLXNA3', 'PLXNA4', 'PLXNB1', 'POGZ', 'POLR2A', 'POLR3A', 'POT1', 'POU3F3', 'PPP3CA', 'PREX1', 'PRKCB', 'PRKD1', 'PRKD2', 'PRKN', 'PRR12', 'PSMD11', 'PSMD12', 'PTEN', 'QRICH1', 'RAD21', 'RAI1', 'RAPGEF4', 'RBM27', 'RELN', 'RERE', 'RFX3', 'RFX4', 'RFX7', 'RGS7', 'RHOXF1', 'RLIM', 'RNF25', 'RORA', 'RORB', 'RUNX1T1', 'SATB1', 'SATB2', 'SETBP1', 'SETDB1', 'SETDB2', 'SHOX', 'SIK1', 'SIN3A', 'SIN3B', 'SKI', 'SLC4A10', 'SMAD4', 'SMARCA2', 'SMARCA4', 'SMARCC2', 'SMC3', 'SON', 'SOX5', 'SOX6', 'SPEN', 'SRCAP', 'SSRP1', 'STAG1', 'SUPT16H', 'TAF1', 'TAF1C', 'TAF4', 'TAF6', 'TBR1', 'TBX1', 'TBX22', 'TCEAL1', 'TCF20', 'TCF4', 'TCF7L2', 'TERF2', 'TET2', 'TET3', 'TFE3', 'THRA', 'TLE3', 'TOP2B', 'TRAPPC9', 'TRIM23', 'TRIM32', 'TRIM33', 'TRRAP', 'TSC2', 'TSHZ1', 'TSHZ3', 'USP7', 'VDR', 'VEZF1', 'WNT1', 'WWOX', 'YEATS2', 'YY1', 'ZBTB16', 'ZBTB18', 'ZBTB20', 'ZBTB21', 'ZBTB7A', 'ZC3H11A', 'ZC3H4', 'ZFYVE26', 'ZMYM2', 'ZMYM3', 'ZNF18', 'ZNF292', 'ZNF385B', 'ZNF462', 'ZNF517', 'ZNF548', 'ZNF559', 'ZNF626', 'ZNF711', 'ZNF713', 'ZNF774', 'ZNF804A', 'ZNF827']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace email that you use for pubmed login\n",
    "pubmed = PubMed(tool=\"MyTool\", email=\"choyoungb@gmail.com\")  # change to your email\n",
    "\n",
    "# Replace tf with your list of gene names\n",
    "original_file = pd.read_excel('TF.xlsx')\n",
    "tf = original_file.iloc[:, 0].tolist()  # list of the first five genes\n",
    "df = pd.DataFrame(columns=['gene', 'info'])\n",
    "print(tf)\n",
    "len(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139597e6",
   "metadata": {},
   "source": [
    "# GPT Filter X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['gene', 'info'])\n",
    "\n",
    "for gene in tf:\n",
    "    fullName = gene_fullName(gene)\n",
    "    fullName_list = re.findall(r'[A-Za-z0-9]+', fullName)\n",
    "    print(fullName_list)\n",
    "    \n",
    "    if len(fullName_list) > 1:\n",
    "        fullName_string = ' '.join(fullName_list)\n",
    "    else:\n",
    "        fullName_string = fullName_list[0]\n",
    "    \n",
    "    query_list = gene_to_search(gene, fullName_string)\n",
    "    i=0\n",
    "    \n",
    "    for search_phrase in query_list:\n",
    "        print(i, search_phrase)\n",
    "        \n",
    "        search_df = search_query(search_phrase, fullName_list, gene, i)\n",
    "        i += 1\n",
    "        \n",
    "        for index, row in search_df.iterrows():\n",
    "            existing_row = df[df['info'] == row['info']]\n",
    "\n",
    "            if not existing_row.empty:\n",
    "                if gene not in existing_row['gene'].values[0]:\n",
    "                    df.loc[existing_row.index, 'gene'] += f\", {gene}\"\n",
    "            else:\n",
    "                row['gene'] = gene\n",
    "                df = pd.concat([df, row.to_frame().T], ignore_index=True)       \n",
    "                \n",
    "print(\"Dataframe complete\")\n",
    "                \n",
    "doc = Document()\n",
    "table = doc.add_table(rows=2 * len(df), cols=1)\n",
    "row_idx = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    table.cell(row_idx, 0).text = str(row[\"gene\"])\n",
    "    row_idx += 1\n",
    "    abstract = row[\"info\"].split(\"Title: \")[-1].split(\"\\n\\n\", 1)[1]\n",
    "\n",
    "    wordcloud = generate_wordcloud(abstract, nlp1, nlp2, nlp3)\n",
    "\n",
    "    img_path = f\"wordcloud_{index}.png\"\n",
    "    if isinstance(wordcloud, WordCloud):\n",
    "        wordcloud.to_file(img_path)\n",
    "    else: \n",
    "        wordcloud.save(img_path)\n",
    "\n",
    "\n",
    "    table.cell(row_idx, 0).text = str(row[\"info\"])\n",
    "    paragraph = table.cell(row_idx, 0).paragraphs[0]\n",
    "    run = paragraph.add_run()\n",
    "    \n",
    "    \n",
    "    run.add_picture(img_path, width=Inches(6))\n",
    "    os.remove(img_path)\n",
    "\n",
    "    row_idx += 1\n",
    "\n",
    "\n",
    "doc.save(\"output.docx\")\n",
    "print(\"WordFile Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab82396",
   "metadata": {},
   "source": [
    "# Convert wordclouded docx to no-wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bd92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('no-worldcloud doc generation')\n",
    "\n",
    "# Read the existing 'output.docx' file\n",
    "input_doc = Document('output.docx')\n",
    "\n",
    "# Create a new docx file\n",
    "output_doc = Document()\n",
    "\n",
    "# Add a table with the same size as the input table\n",
    "table = output_doc.add_table(rows=len(input_doc.tables[0].rows), cols=1)\n",
    "\n",
    "# Iterate through the cells of the input table\n",
    "for idx, cell in enumerate(input_doc.tables[0]._cells):\n",
    "\n",
    "    # Copy the text from the input cell to the output cell\n",
    "    table._cells[idx].text = cell.text\n",
    "\n",
    "    # Remove images from the output cell\n",
    "    for paragraph in table._cells[idx].paragraphs:\n",
    "        for run in paragraph.runs:\n",
    "            if run.element.rPr:\n",
    "                for child in run.element.rPr.getchildren():\n",
    "                    if child.tag.endswith('blip'):\n",
    "                        child.getparent().getparent().getparent().getparent().remove(run.element.rPr)\n",
    "                        \n",
    "print('complete')\n",
    "\n",
    "# Save the new docx file\n",
    "output_doc.save('output_no_images.docx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
